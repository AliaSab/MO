"""
–ì–ª–∞–≤–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤.
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ 9 —ç—Ç–∞–ø–æ–≤ –∑–∞–¥–∞–Ω–∏—è.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
import os
os.environ['TCL_LIBRARY'] = "C:/Program Files/Python313/tcl/tcl8.6"
os.environ['TK_LIBRARY'] = "C:/Program Files/Python313/tcl/tk8.6"
import time
import warnings
warnings.filterwarnings('ignore')

# –ò–º–ø–æ—Ä—Ç—ã –Ω–∞—à–∏—Ö –º–æ–¥—É–ª–µ–π
from feature_engineering import FeatureEngineer
from validation import DataValidator
from hyperparameter_tuning import HyperparameterTuner
from forecasting_strategies import DirectStrategy, RecursiveStrategy, MultiOutputStrategy, DirRecStrategy
from models import create_all_models, BaselineModels, ModelTrainer
from diagnostics import ModelDiagnostics
from evaluation import MetricsCalculator, ModelEvaluator, DieboldMarianoTest
from advanced_techniques import AdvancedTechniques

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
try:
    from alternative_models import AlternativeTimeSeriesModels
    ALTERNATIVE_MODELS_AVAILABLE = True
except ImportError:
    ALTERNATIVE_MODELS_AVAILABLE = False
    print("–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã")

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ AutoGluon
try:
    from autogluon_integration import AutoGluonWrapper, compare_autogluon_vs_custom, create_autogluon_recommendations
    AUTOGLUON_AVAILABLE = True
except ImportError:
    AUTOGLUON_AVAILABLE = False
    print("AutoGluon –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ matplotlib –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
plt.rcParams['font.family'] = 'DejaVu Sans'
sns.set_style("whitegrid")


def load_data(file_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ CSV —Ñ–∞–π–ª–∞."""
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {file_path}...")
    df = pd.read_csv(file_path)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç
    if 'Date' in df.columns:
        df['Date'] = pd.to_datetime(df['Date'], utc=True)
        if df['Date'].dt.tz is not None:
            df['Date'] = df['Date'].dt.tz_localize(None)
    
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫")
    return df


def prepare_time_series(df, target_col='Weekly_Sales', date_col='Date', 
                        group_cols=None):
    """
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥.
    
    –ï—Å–ª–∏ –µ—Å—Ç—å –≥—Ä—É–ø–ø–∏—Ä—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ (Store, Dept), –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º –ø–æ –Ω–∏–º.
    """
    if group_cols is None:
        group_cols = []
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å –≥—Ä—É–ø–ø–∏—Ä—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏, –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º
    if group_cols and all(col in df.columns for col in group_cols):
        print(f"–ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ {group_cols}...")
        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
        agg_dict = {target_col: 'sum'}
        if 'IsHoliday' in df.columns:
            agg_dict['IsHoliday'] = 'max'  # –ë–µ—Ä–µ–º max (True –µ—Å–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω True)
        
        df_grouped = df.groupby(group_cols + [date_col]).agg(agg_dict).reset_index()
        df_grouped = df_grouped.sort_values(date_col)
        date_index = pd.DatetimeIndex(df_grouped[date_col])
        series = df_grouped[target_col]
        is_holiday = df_grouped['IsHoliday'] if 'IsHoliday' in df_grouped.columns else None
    else:
        # –ü—Ä–æ—Å—Ç–æ–π —Å–ª—É—á–∞–π - –æ–¥–∏–Ω —Ä—è–¥
        df_sorted = df.sort_values(date_col)
        date_index = pd.DatetimeIndex(df_sorted[date_col])
        series = df_sorted[target_col]
        is_holiday = df_sorted['IsHoliday'] if 'IsHoliday' in df_sorted.columns else None
    
    return series, date_index, is_holiday


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞."""
    print("=" * 80)
    print("–ü–ê–ô–ü–õ–ê–ô–ù –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø –í–†–ï–ú–ï–ù–ù–´–• –†–Ø–î–û–í")
    print("=" * 80)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    DATA_PATH = 'New_final.csv'
    TARGET_COL = 'Weekly_Sales'
    DATE_COL = 'Date'
    HORIZON = 7  # –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è
    RESULTS_DIR = 'results'
    
    # –î–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏: –µ—Å–ª–∏ True, —Ç–æ –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º –ø–æ Store –∏ Dept
    # –ï—Å–ª–∏ False, —Ç–æ –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π Store –∏ Dept –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    AGGREGATE_BY_GROUP = False
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    os.makedirs(RESULTS_DIR, exist_ok=True)
    
    # ========== –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ==========
    print("\n[–≠–¢–ê–ü 0] –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    df = load_data(DATA_PATH)
    
    # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –±–µ—Ä–µ–º –æ–¥–∏–Ω Store –∏ Dept, –∏–ª–∏ –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º
    if AGGREGATE_BY_GROUP:
        series, date_index, is_holiday = prepare_time_series(
            df, TARGET_COL, DATE_COL, group_cols=['Store', 'Dept']
        )
    else:
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π Store –∏ Dept –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        first_store = df['Store'].iloc[0] if 'Store' in df.columns else None
        first_dept = df['Dept'].iloc[0] if 'Dept' in df.columns else None
        if first_store is not None and first_dept is not None:
            df_filtered = df[(df['Store'] == first_store) & (df['Dept'] == first_dept)]
            print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º Store={first_store}, Dept={first_dept}")
        else:
            df_filtered = df
        series, date_index, is_holiday = prepare_time_series(
            df_filtered, TARGET_COL, DATE_COL, group_cols=None
        )
    
    print(f"–î–ª–∏–Ω–∞ —Ä—è–¥–∞: {len(series)}")
    print(f"–ü–µ—Ä–∏–æ–¥: {date_index.min()} - {date_index.max()}")
    
    # ========== –≠–¢–ê–ü 1: –ò–ù–ñ–ò–ù–ò–†–ò–ù–ì –ü–†–ò–ó–ù–ê–ö–û–í ==========
    print("\n[–≠–¢–ê–ü 1] –ò–Ω–∂–∏–Ω–∏—Ä–∏–Ω–≥ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
    feature_engineer = FeatureEngineer()
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
    X, y_transformed, transform_info = feature_engineer.create_all_features(
        series, date_index, is_holiday, apply_log=True, apply_boxcox=False
    )
    
    print(f"–°–æ–∑–¥–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X.shape[1]}")
    print(f"–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è: {transform_info}")
    
    # ========== –≠–¢–ê–ü 2: –í–ê–õ–ò–î–ê–¶–ò–Ø –ò –†–ê–ó–ë–ò–ï–ù–ò–ï ==========
    print("\n[–≠–¢–ê–ü 2] –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
    validator = DataValidator(train_ratio=0.6, val_ratio=0.2, test_ratio=0.2)
    
    # –•—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ
    X_train, X_val, X_test = validator.chronological_split(X, date_index)
    y_train, y_val, y_test = validator.chronological_split(y_transformed, date_index)
    date_train, date_val, date_test = validator.chronological_split(date_index, date_index)
    
    print(f"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}")
    
    # TimeSeriesSplit –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
    tscv = validator.create_time_series_split(n_splits=5, max_train_size=365)
    
    # ========== –≠–¢–ê–ü 3: –ü–û–î–ë–û–† –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–û–í ==========
    print("\n[–≠–¢–ê–ü 3] –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...")
    tuner = HyperparameterTuner(cv=tscv)
    
    # GridSearch –¥–ª—è Ridge
    from sklearn.linear_model import Ridge
    ridge_param_grid = {'alpha': [0.1, 1.0, 10.0, 100.0]}
    best_ridge = tuner.grid_search_linear(
        Ridge(), X_train, y_train, ridge_param_grid
    )
    print(f"–õ—É—á—à–∏–π Ridge: alpha={best_ridge.alpha}")
    
    # Optuna –¥–ª—è LightGBM (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
    try:
        lgbm_params, best_lgbm = tuner.optuna_tune_lgbm(
            X_train, y_train, n_trials=50  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        )
        print(f"–õ—É—á—à–∏–π LightGBM: {lgbm_params}")
    except Exception as e:
        print(f"Optuna tuning –ø—Ä–æ–ø—É—â–µ–Ω: {e}")
        best_lgbm = None
    
    # ========== –≠–¢–ê–ü 4: –°–¢–†–ê–¢–ï–ì–ò–ò –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø ==========
    print("\n[–≠–¢–ê–ü 4] –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è...")
    
    # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
    from sklearn.linear_model import LinearRegression
    base_model = LinearRegression()
    
    strategies = {
        'Direct': DirectStrategy(base_model, horizon=HORIZON),
        'Recursive': RecursiveStrategy(base_model, horizon=HORIZON),
        'MultiOutput': MultiOutputStrategy(base_model, horizon=HORIZON),
        'DirRec': DirRecStrategy(base_model, horizon=HORIZON, window_size=3)
    }
    
    # –û–±—É—á–∞–µ–º –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    strategy_results = {}
    for name, strategy in strategies.items():
        print(f"  –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é: {name}")
        try:
            strategy.fit(X_train, y_train)
            preds = strategy.predict(X_val[:100])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            strategy_results[name] = preds
        except Exception as e:
            print(f"    –û—à–∏–±–∫–∞: {e}")
    
    # ========== –≠–¢–ê–ü 5: –ü–û–°–¢–†–û–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô ==========
    print("\n[–≠–¢–ê–ü 5] –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
    
    # –°–æ–∑–¥–∞–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏
    all_models = create_all_models()
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
    if best_ridge:
        all_models['Ridge_tuned'] = best_ridge
    if best_lgbm:
        all_models['LightGBM_tuned'] = best_lgbm
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏
    trainer = ModelTrainer()
    for name, model in all_models.items():
        trainer.add_model(name, model)
    
    train_start = time.time()
    trained_models = trainer.train_all(X_train, y_train)
    train_time = time.time() - train_start
    
    print(f"–û–±—É—á–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(trained_models)}")
    print(f"–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {train_time:.2f} —Å–µ–∫")
    
    # –ë–µ–π–∑–ª–∞–π–Ω—ã
    baseline_preds = {}
    baseline_preds['Naive'] = BaselineModels.naive_forecast(y_train, HORIZON)
    baseline_preds['SeasonalNaive'] = BaselineModels.seasonal_naive_forecast(y_train, 7, HORIZON)
    baseline_preds['MovingAverage'] = BaselineModels.moving_average_forecast(y_train, 7, HORIZON)
    baseline_preds['LinearTrend'] = BaselineModels.linear_trend_forecast(y_train, HORIZON)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ HORIZON —Ç–æ—á–µ–∫ –¥–ª—è –æ–¥–Ω–æ—à–∞–≥–æ–≤–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞)
    # –î–ª—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞ –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏–∑ —ç—Ç–∞–ø–∞ 4
    predictions_val = trainer.predict_all(X_val[:min(len(X_val), 100)])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    
    # ========== –≠–¢–ê–ü 6: –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ú–û–î–ï–õ–ï–ô ==========
    print("\n[–≠–¢–ê–ü 6] –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –º–æ–¥–µ–ª–µ–π...")
    
    diagnostics = ModelDiagnostics()
    
    # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–ª—è —Ç–æ–ø-3 –º–æ–¥–µ–ª–µ–π
    top_models = list(predictions_val.keys())[:3]
    
    for model_name in top_models:
        if model_name in predictions_val:
            y_pred = predictions_val[model_name]
            if len(y_pred) > 0:
                y_true_slice = y_val.iloc[:len(y_pred)] if isinstance(y_val, pd.Series) else y_val[:len(y_pred)]
                residuals = diagnostics.calculate_residuals(y_true_slice, y_pred, model_name)
                
                # ACF –æ—Å—Ç–∞—Ç–∫–æ–≤
                try:
                    fig = diagnostics.plot_acf(residuals, model_name)
                    fig.savefig(f'{RESULTS_DIR}/acf_{model_name}.png', dpi=150, bbox_inches='tight')
                    plt.close(fig)
                except Exception as e:
                    print(f"  –û—à–∏–±–∫–∞ ACF –¥–ª—è {model_name}: {e}")
                
                # Feature importance
                if model_name in trained_models:
                    model = trained_models[model_name]
                    importance = diagnostics.get_feature_importance(model, X_train.columns.tolist(), model_name)
                    if importance is not None:
                        fig = diagnostics.plot_feature_importance(importance, model_name)
                        fig.savefig(f'{RESULTS_DIR}/feature_importance_{model_name}.png', dpi=150, bbox_inches='tight')
                        plt.close(fig)
    
    # ========== –≠–¢–ê–ü 7: –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê ==========
    print("\n[–≠–¢–ê–ü 7] –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞...")
    
    # –î–ª—è MASE –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–¥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏), –µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω—ã
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª–∞ –ª–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è
    if transform_info.get('log', False):
        # –û–±—Ä–∞—Ç–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ MASE
        y_train_for_mase = feature_engineer.inverse_log_transform(y_train)
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–¥–æ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞–Ω–∏—è) –¥–ª—è MASE")
    else:
        y_train_for_mase = y_train
    
    evaluator = ModelEvaluator(y_train=y_train_for_mase, seasonality=7)
    
    # –û—Ü–µ–Ω–∏–≤–∞–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏
    all_predictions = {}
    for name, pred in predictions_val.items():
        if len(pred) > 0:
            y_true_slice = y_val.iloc[:len(pred)] if isinstance(y_val, pd.Series) else y_val[:len(pred)]
            all_predictions[name] = pred
    
    # –î–æ–±–∞–≤–ª—è–µ–º –±–µ–π–∑–ª–∞–π–Ω—ã (–¥–ª—è –æ–¥–Ω–æ—à–∞–≥–æ–≤–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞ –±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)
    for name, baseline_pred in baseline_preds.items():
        if len(baseline_pred) > 0:
            # –î–ª—è –æ–¥–Ω–æ—à–∞–≥–æ–≤–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –±–µ–π–∑–ª–∞–π–Ω–∞
            all_predictions[name] = np.full(len(y_val[:min(len(y_val), 100)]), baseline_pred[0])
    
    # –ú–µ—Ç—Ä–∏–∫–∏ (–±–µ—Ä–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Å—Ä–µ–∑ y_val)
    max_len = max([len(pred) for pred in all_predictions.values()] + [len(y_val)])
    y_val_slice = y_val.iloc[:min(max_len, len(y_val))] if isinstance(y_val, pd.Series) else y_val[:min(max_len, len(y_val))]
    metrics_df = evaluator.evaluate_all_models(y_val_slice, all_predictions)
    print("\n–ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–µ–π:")
    print(metrics_df.to_string())
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    metrics_df.to_csv(f'{RESULTS_DIR}/metrics.csv', index=False)
    metrics_df.to_json(f'{RESULTS_DIR}/metrics.json', orient='records', indent=2)
    
    # Diebold-Mariano —Ç–µ—Å—Ç
    if len(all_predictions) >= 2:
        model_names = list(all_predictions.keys())
        # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–ª–∏–Ω—ã –¥–ª—è DM —Ç–µ—Å—Ç–∞
        min_len = min([len(all_predictions[k]) for k in model_names[:5]] + [len(y_val_slice)])
        dm_predictions = {k: all_predictions[k][:min_len] for k in model_names[:5]}
        dm_results = evaluator.compare_models_dm(
            y_val_slice[:min_len], 
            dm_predictions
        )
        dm_results.to_csv(f'{RESULTS_DIR}/dm_test.csv')
        print("\nDiebold-Mariano —Ç–µ—Å—Ç:")
        print(dm_results.to_string())
    
    # ========== –≠–¢–ê–ü 7.5: AUTOGLUON ==========
    autogluon_predictions = {}
    autogluon_training_times = {}
    autogluon_leaderboards = {}
    
    if AUTOGLUON_AVAILABLE:
        print("\n" + "=" * 80)
        print("[–≠–¢–ê–ü 7.5] AUTOGLUON TIMESERIES")
        print("=" * 80)
        
        try:
            # –°–æ–∑–¥–∞–µ–º wrapper
            ag_wrapper = AutoGluonWrapper(
                prediction_length=HORIZON,
                eval_metric="MAE",
                freq="W"
            )
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è AutoGluon
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º train –∏ val –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, —Ç–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ test
            train_val_series = pd.concat([y_train, y_val])
            train_val_dates = pd.concat([
                pd.Series(date_train),
                pd.Series(date_val)
            ])
            
            ag_train_data = ag_wrapper.prepare_data(
                train_val_series,
                pd.DatetimeIndex(train_val_dates)
            )
            
            # –û–±—É—á–∞–µ–º —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø—Ä–µ—Å–µ—Ç–∞–º–∏
            presets_to_test = ["medium_quality", "high_quality"]  # best_quality –∑–∞–π–º–µ—Ç –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—Ä–µ–º—è –Ω–∞ –∫–∞–∂–¥—ã–π –ø—Ä–µ—Å–µ—Ç (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
            time_limit_per_preset = 300  # 5 –º–∏–Ω—É—Ç –Ω–∞ –ø—Ä–µ—Å–µ—Ç
            
            ag_wrapper.fit_multiple_presets(
                ag_train_data,
                presets=presets_to_test,
                time_limit_per_preset=time_limit_per_preset
            )
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ—Å–µ—Ç–∞
            for preset in presets_to_test:
                if preset in ag_wrapper.predictors:
                    try:
                        predictions = ag_wrapper.predict(preset=preset, quantile_levels=[0.1, 0.9])
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º mean predictions
                        if isinstance(predictions, pd.DataFrame):
                            if 'mean' in predictions.columns:
                                pred_values = predictions['mean'].values
                            else:
                                pred_values = predictions.values.flatten()
                        else:
                            pred_values = predictions
                        
                        # –ü–æ–≤—Ç–æ—Ä—è–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –≤—Å–µ–π –¥–ª–∏–Ω—ã test (–¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)
                        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ AutoGluon –¥–∞–µ—Ç h-—à–∞–≥–æ–≤—ã–π –ø—Ä–æ–≥–Ω–æ–∑
                        test_len = len(y_test)
                        if len(pred_values) < test_len:
                            # –†–∞—Å—à–∏—Ä—è–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
                            pred_extended = np.concatenate([
                                pred_values,
                                np.full(test_len - len(pred_values), pred_values[-1])
                            ])
                        else:
                            pred_extended = pred_values[:test_len]
                        
                        autogluon_predictions[f'AutoGluon_{preset}'] = pred_extended
                        autogluon_training_times[preset] = ag_wrapper.training_times[preset]
                        
                        # –ü–æ–ª—É—á–∞–µ–º leaderboard
                        leaderboard = ag_wrapper.get_leaderboard(preset)
                        if leaderboard is not None:
                            autogluon_leaderboards[preset] = leaderboard
                            
                            print(f"\nüìä Leaderboard –¥–ª—è {preset}:")
                            print(leaderboard[['model', 'score_val']].head(5).to_string())
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º leaderboard
                            leaderboard.to_csv(f'{RESULTS_DIR}/autogluon_leaderboard_{preset}.csv', index=False)
                    
                    except Exception as e:
                        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≥–Ω–æ–∑–µ –¥–ª—è {preset}: {e}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º AutoGluon –ø—Ä–æ–≥–Ω–æ–∑—ã –≤ –æ–±—â–∏–π –ø—É–ª –¥–ª—è –æ—Ü–µ–Ω–∫–∏
            for name, pred in autogluon_predictions.items():
                all_predictions[name] = pred
            
            # Backtesting –≤–∞–ª–∏–¥–∞—Ü–∏—è
            print("\n" + "-" * 80)
            print("BACKTESTING –í–ê–õ–ò–î–ê–¶–ò–Ø AUTOGLUON")
            print("-" * 80)
            
            for preset in presets_to_test:
                if preset in ag_wrapper.predictors:
                    try:
                        backtesting_results = ag_wrapper.backtesting(
                            ag_train_data,
                            num_windows=3,
                            preset=preset
                        )
                        
                        print(f"\n‚úÖ Backtesting –¥–ª—è {preset} –∑–∞–≤–µ—Ä—à–µ–Ω")
                        backtesting_results.to_csv(f'{RESULTS_DIR}/autogluon_backtesting_{preset}.csv', index=False)
                    
                    except Exception as e:
                        print(f"‚ö†Ô∏è Backtesting –¥–ª—è {preset} –ø—Ä–æ–ø—É—â–µ–Ω: {e}")
            
            # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Å AutoGluon –º–æ–¥–µ–ª—è–º–∏
            print("\n" + "-" * 80)
            print("–û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê –° AUTOGLUON")
            print("-" * 80)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º test set –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏
            y_test_slice = y_test.iloc[:min(len(y_test), 100)] if isinstance(y_test, pd.Series) else y_test[:min(len(y_test), 100)]
            
            # –û–±—Ä–µ–∑–∞–µ–º –≤—Å–µ –ø—Ä–æ–≥–Ω–æ–∑—ã –¥–æ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –¥–ª–∏–Ω—ã
            for name in list(all_predictions.keys()):
                if len(all_predictions[name]) > len(y_test_slice):
                    all_predictions[name] = all_predictions[name][:len(y_test_slice)]
            
            metrics_df_with_ag = evaluator.evaluate_all_models(y_test_slice, all_predictions)
            
            print("\n–ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–µ–π (–≤–∫–ª—é—á–∞—è AutoGluon):")
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø-10
            print(metrics_df_with_ag.sort_values('MASE').head(10).to_string())
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            metrics_df_with_ag.to_csv(f'{RESULTS_DIR}/metrics_with_autogluon.csv', index=False)
            metrics_df_with_ag.to_json(f'{RESULTS_DIR}/metrics_with_autogluon.json', orient='records', indent=2)
            
            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ AutoGluon vs Custom
            print("\n" + "=" * 80)
            print("–°–†–ê–í–ù–ï–ù–ò–ï AUTOGLUON VS –ö–ê–°–¢–û–ú–ù–´–ï –ú–û–î–ï–õ–ò")
            print("=" * 80)
            
            # –ì–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            custom_predictions_for_comparison = {k: v for k, v in all_predictions.items() 
                                                 if not k.startswith('AutoGluon_')}
            
            comparison_df = compare_autogluon_vs_custom(
                autogluon_preds=autogluon_predictions,
                custom_preds=custom_predictions_for_comparison,
                y_true=y_test_slice,
                autogluon_time=autogluon_training_times,
                custom_time=train_time
            )
            
            print("\n–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞:")
            print(comparison_df.to_string())
            comparison_df.to_csv(f'{RESULTS_DIR}/autogluon_vs_custom.csv', index=False)
            
            # –°–æ–∑–¥–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            recommendations = create_autogluon_recommendations(comparison_df)
            
            print("\n" + "=" * 80)
            print("–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ")
            print("=" * 80)
            
            print("\nüìå –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞:")
            for key, value in recommendations['summary'].items():
                print(f"  {key}: {value}")
            
            print("\n‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å AutoGluon –∫–æ–≥–¥–∞:")
            for rec in recommendations['use_autogluon_when']:
                print(f"  - {rec}")
            
            print("\n‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞—Å—Ç–æ–º–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–æ–≥–¥–∞:")
            for rec in recommendations['use_custom_when']:
                print(f"  - {rec}")
            
            print(f"\nüöÄ –°—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞: {recommendations['production_strategy'].get('approach', 'N/A')}")
            print(f"   {recommendations['production_strategy'].get('description', '')}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            with open(f'{RESULTS_DIR}/autogluon_recommendations.json', 'w', encoding='utf-8') as f:
                json.dump(recommendations, f, indent=2, ensure_ascii=False)
            
            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–µ—Å–µ—Ç–æ–≤ AutoGluon
            preset_comparison = ag_wrapper.compare_presets()
            if preset_comparison is not None:
                print("\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–µ—Å–µ—Ç–æ–≤ AutoGluon:")
                print(preset_comparison.to_string())
                preset_comparison.to_csv(f'{RESULTS_DIR}/autogluon_presets_comparison.csv', index=False)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º metrics_df –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            metrics_df = metrics_df_with_ag
            
        except Exception as e:
            print(f"\n‚ùå –û–®–ò–ë–ö–ê –≤ AutoGluon: {e}")
            import traceback
            traceback.print_exc()
    else:
        print("\n‚ö†Ô∏è AutoGluon –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–∞–ø")
        print("   –î–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏: pip install autogluon.timeseries")
    
    # ========== –≠–¢–ê–ü 8: –ü–†–û–î–í–ò–ù–£–¢–´–ï –¢–ï–•–ù–ò–ö–ò ==========
    print("\n[–≠–¢–ê–ü 8] –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏...")
    
    advanced = AdvancedTechniques()
    
    # –ê–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
    if len(all_predictions) >= 2:
        mase_scores = dict(zip(metrics_df['model'], metrics_df['MASE']))
        ensemble_pred = advanced.create_ensemble(all_predictions, 'weighted_mase', mase_scores)
        print("–°–æ–∑–¥–∞–Ω –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å")
    
    # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ AutoGluon –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)
    if ALTERNATIVE_MODELS_AVAILABLE:
        print("\n[–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û] –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π...")
        alt_predictions = {}
        try:
            alt_models = AlternativeTimeSeriesModels()
            
            # StatsForecast
            if 'statsforecast' in alt_models.available_libs:
                try:
                    train_df = alt_models.prepare_data_for_statsforecast(y_train)
                    alt_predictions = alt_models.fit_statsforecast_models(train_df, horizon=HORIZON)
                    for name, pred in alt_predictions.items():
                        if len(pred) > 0:
                            all_predictions[name] = np.full(len(y_val_slice), pred[0] if len(pred) > 0 else y_train.iloc[-1])
                    print(f"  –î–æ–±–∞–≤–ª–µ–Ω–æ –º–æ–¥–µ–ª–µ–π StatsForecast: {len(alt_predictions)}")
                except Exception as e:
                    print(f"  –û—à–∏–±–∫–∞ StatsForecast: {e}")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
            if alt_predictions:
                metrics_df_updated = evaluator.evaluate_all_models(y_val_slice, all_predictions)
                metrics_df = metrics_df_updated
                print(f"  –í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö: {len(metrics_df)}")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏: {e}")
    
    # ========== –≠–¢–ê–ü 9: –ò–¢–û–ì–û–í–´–ô –ê–ù–ê–õ–ò–ó ==========
    print("\n[–≠–¢–ê–ü 9] –ò—Ç–æ–≥–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑...")
    
    # –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    if 'MASE' in metrics_df.columns:
        metrics_df_sorted = metrics_df.sort_values('MASE')
        print("\n–†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ MASE:")
        print(metrics_df_sorted[['model', 'MASE', 'MAE', 'RMSE']].head(10).to_string())
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–ø-3 –º–æ–¥–µ–ª–µ–π
        top_3_models = metrics_df_sorted.head(3)['model'].tolist()
        
        fig, axes = plt.subplots(3, 1, figsize=(14, 12))
        for idx, model_name in enumerate(top_3_models):
            if model_name in all_predictions:
                y_pred = all_predictions[model_name]
                y_true_slice = y_val.iloc[:len(y_pred)] if isinstance(y_val, pd.Series) else y_val[:len(y_pred)]
                
                # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–ª–∏–Ω—ã
                min_len = min(len(y_pred), len(y_true_slice))
                y_pred_aligned = y_pred[:min_len]
                y_true_aligned = y_true_slice.iloc[:min_len] if isinstance(y_true_slice, pd.Series) else y_true_slice[:min_len]
                
                axes[idx].plot(y_true_aligned.values if isinstance(y_true_aligned, pd.Series) else y_true_aligned, 
                              label='–§–∞–∫—Ç', alpha=0.7)
                axes[idx].plot(y_pred_aligned, label='–ü—Ä–æ–≥–Ω–æ–∑', alpha=0.7)
                mase_val = metrics_df_sorted[metrics_df_sorted["model"]==model_name]["MASE"].values[0] if len(metrics_df_sorted[metrics_df_sorted["model"]==model_name]) > 0 else 0
                axes[idx].set_title(f'{model_name} (MASE: {mase_val:.4f})')
                axes[idx].legend()
                axes[idx].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{RESULTS_DIR}/top3_models.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    summary = {
        'total_models': len(trained_models),
        'best_model': metrics_df_sorted.iloc[0]['model'] if 'MASE' in metrics_df.columns else None,
        'best_mase': metrics_df_sorted.iloc[0]['MASE'] if 'MASE' in metrics_df.columns else None,
        'train_time': train_time,
        'n_features': X.shape[1],
        'train_size': len(X_train),
        'val_size': len(X_val),
        'test_size': len(X_test)
    }
    
    with open(f'{RESULTS_DIR}/summary.json', 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print("\n" + "=" * 80)
    print("–ü–ê–ô–ü–õ–ê–ô–ù –ó–ê–í–ï–†–®–ï–ù")
    print("=" * 80)
    print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: {RESULTS_DIR}/")
    print(f"–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {summary['best_model']}")
    print(f"–õ—É—á—à–∏–π MASE: {summary['best_mase']:.6f}")


if __name__ == '__main__':
    main()

