"""
–ú–æ–¥—É–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.
–í–∫–ª—é—á–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±—É—á–µ–Ω–∏—è, —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é, early stopping, gradient clipping.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from typing import Dict, List, Optional, Tuple, Callable
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')


class TimeSeriesDataset(Dataset):
    """Dataset –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤."""
    
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
        
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


class ModelTrainer:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è."""
    
    def __init__(self, model, device='cpu', loss_fn='mse', optimizer='adam',
                 lr=1e-3, weight_decay=1e-4, gradient_clip=1.0,
                 early_stopping_patience=15, reduce_lr_patience=10,
                 label_smoothing=0.0):
        """
        Args:
            model: PyTorch –º–æ–¥–µ–ª—å
            device: 'cpu' –∏–ª–∏ 'cuda'
            loss_fn: 'mse', 'mae', 'huber' –∏–ª–∏ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è
            optimizer: 'adam', 'adamw', 'radam'
            lr: learning rate
            weight_decay: weight decay –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
            gradient_clip: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞
            early_stopping_patience: —Ç–µ—Ä–ø–µ–Ω–∏–µ –¥–ª—è early stopping
            reduce_lr_patience: —Ç–µ—Ä–ø–µ–Ω–∏–µ –¥–ª—è ReduceLROnPlateau
            label_smoothing: –ø–∞—Ä–∞–º–µ—Ç—Ä label smoothing
        """
        self.model = model.to(device)
        self.device = device
        self.label_smoothing = label_smoothing
        
        # –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
        if loss_fn == 'mse':
            self.criterion = nn.MSELoss()
        elif loss_fn == 'mae':
            self.criterion = nn.L1Loss()
        elif loss_fn == 'huber':
            self.criterion = nn.HuberLoss()
        elif loss_fn == 'mse+mae':
            self.criterion = lambda pred, target: nn.MSELoss()(pred, target) + nn.L1Loss()(pred, target)
        elif loss_fn == 'mse+huber':
            self.criterion = lambda pred, target: nn.MSELoss()(pred, target) + nn.HuberLoss()(pred, target)
        else:
            self.criterion = nn.MSELoss()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        params = list(model.parameters())
        self.has_trainable_params = len(params) > 0
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã)
        if self.has_trainable_params:
            if optimizer == 'adam':
                self.optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
            elif optimizer == 'adamw':
                self.optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
            elif optimizer == 'radam':
                try:
                    from torch_optimizer import RAdam
                    self.optimizer = RAdam(model.parameters(), lr=lr, weight_decay=weight_decay)
                except ImportError:
                    warnings.warn("RAdam –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Adam")
                    self.optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
            else:
                self.optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
            
            # Scheduler
            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer, mode='min', factor=0.5, patience=reduce_lr_patience
            )
        else:
            # –î–ª—è –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, Naive)
            self.optimizer = None
            self.scheduler = None
        
        # Early stopping
        self.early_stopping_patience = early_stopping_patience
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        self.best_model_state = None
        self.best_epoch = 0
        
        # Gradient clipping
        self.gradient_clip = gradient_clip
        
        # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        self.train_losses = []
        self.val_losses = []
        
    def train_epoch(self, train_loader):
        """–û–¥–Ω–∞ —ç–ø–æ—Ö–∞ –æ–±—É—á–µ–Ω–∏—è."""
        if not self.has_trainable_params:
            # –î–ª—è –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–æ—Å—Ç–æ –≤—ã—á–∏—Å–ª—è–µ–º loss
            self.model.eval()
            total_loss = 0
            with torch.no_grad():
                for X_batch, y_batch in train_loader:
                    X_batch = X_batch.to(self.device)
                    y_batch = y_batch.to(self.device)
                    predictions = self.model(X_batch)
                    loss = self.criterion(predictions, y_batch)
                    total_loss += loss.item()
            return total_loss / len(train_loader)
        
        self.model.train()
        total_loss_original = 0  # Loss –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å validation
        n_batches = 0
        
        for X_batch, y_batch in train_loader:
            X_batch = X_batch.to(self.device)
            y_batch = y_batch.to(self.device)
            
            # Forward pass
            self.optimizer.zero_grad()
            predictions = self.model(X_batch)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞)
            if n_batches == 0:
                if predictions.shape != y_batch.shape:
                    import warnings
                    warnings.warn(f"‚ö†Ô∏è –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç: predictions={predictions.shape}, y_batch={y_batch.shape}")
                # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö
                print(f"üîç –û–¢–õ–ê–î–ö–ê train_epoch (–ø–µ—Ä–≤—ã–π –±–∞—Ç—á):")
                print(f"  X_batch: shape={X_batch.shape}, range=[{X_batch.min():.6f}, {X_batch.max():.6f}]")
                print(f"  y_batch: shape={y_batch.shape}, range=[{y_batch.min():.6f}, {y_batch.max():.6f}]")
                print(f"  predictions: shape={predictions.shape}, range=[{predictions.min():.6f}, {predictions.max():.6f}]")
                print(f"  y_batch mean={y_batch.mean():.6f}, std={y_batch.std():.6f}")
                print(f"  predictions mean={predictions.mean():.6f}, std={predictions.std():.6f}")
            
            # –í—ã—á–∏—Å–ª—è–µ–º loss –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è (–¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å validation)
            loss_original = self.criterion(predictions, y_batch)
            total_loss_original += loss_original.item()
            
            # Label smoothing (—Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –Ω–µ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è)
            if self.label_smoothing > 0:
                y_smooth = y_batch * (1 - self.label_smoothing) + y_batch.mean() * self.label_smoothing
                loss = self.criterion(predictions, y_smooth)
            else:
                loss = loss_original
            
            # Backward pass
            loss.backward()
            
            # Gradient clipping
            if self.gradient_clip > 0:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip)
            
            self.optimizer.step()
            n_batches += 1
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º loss –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å validation
        avg_loss = total_loss_original / n_batches if n_batches > 0 else 0.0
        if n_batches > 0:
            print(f"üîç train_epoch –∏—Ç–æ–≥: n_batches={n_batches}, avg_loss={avg_loss:.6f}")
        return avg_loss
    
    def validate(self, val_loader):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏."""
        self.model.eval()
        total_loss = 0
        n_batches = 0
        
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch = X_batch.to(self.device)
                y_batch = y_batch.to(self.device)
                
                predictions = self.model(X_batch)
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞)
                if n_batches == 0:
                    if predictions.shape != y_batch.shape:
                        import warnings
                        warnings.warn(f"‚ö†Ô∏è –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç –≤ validation: predictions={predictions.shape}, y_batch={y_batch.shape}")
                    # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö
                    print(f"üîç –û–¢–õ–ê–î–ö–ê validate (–ø–µ—Ä–≤—ã–π –±–∞—Ç—á):")
                    print(f"  X_batch: shape={X_batch.shape}, range=[{X_batch.min():.6f}, {X_batch.max():.6f}]")
                    print(f"  y_batch: shape={y_batch.shape}, range=[{y_batch.min():.6f}, {y_batch.max():.6f}]")
                    print(f"  predictions: shape={predictions.shape}, range=[{predictions.min():.6f}, {predictions.max():.6f}]")
                    print(f"  y_batch mean={y_batch.mean():.6f}, std={y_batch.std():.6f}")
                    print(f"  predictions mean={predictions.mean():.6f}, std={predictions.std():.6f}")
                
                loss = self.criterion(predictions, y_batch)
                total_loss += loss.item()
                n_batches += 1
        
        avg_loss = total_loss / n_batches if n_batches > 0 else 0.0
        if n_batches > 0:
            print(f"üîç validate –∏—Ç–æ–≥: n_batches={n_batches}, avg_loss={avg_loss:.6f}")
        return avg_loss
    
    def train(self, train_loader, val_loader, epochs=100, verbose=True):
        """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è."""
        for epoch in range(epochs):
            train_loss = self.train_epoch(train_loader)
            val_loss = self.validate(val_loader)
            
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: –≤—ã—á–∏—Å–ª—è–µ–º train loss –≤ —Ä–µ–∂–∏–º–µ eval() –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            if epoch == 0:
                self.model.eval()
                train_loss_eval = 0
                train_batches_eval = 0
                with torch.no_grad():
                    for X_batch, y_batch in train_loader:
                        X_batch = X_batch.to(self.device)
                        y_batch = y_batch.to(self.device)
                        predictions = self.model(X_batch)
                        loss_eval = self.criterion(predictions, y_batch)
                        train_loss_eval += loss_eval.item()
                        train_batches_eval += 1
                train_loss_eval = train_loss_eval / train_batches_eval if train_batches_eval > 0 else 0.0
                self.model.train()
                
                print(f"üîç –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}:")
                print(f"  Train Loss (train mode): {train_loss:.6f}")
                print(f"  Train Loss (eval mode): {train_loss_eval:.6f}")
                print(f"  Val Loss (eval mode): {val_loss:.6f}")
                if abs(train_loss - train_loss_eval) > 0.01:
                    print(f"  ‚ö†Ô∏è –†–ê–ó–ù–ò–¶–ê –º–µ–∂–¥—É train() –∏ eval() —Ä–µ–∂–∏–º–∞–º–∏: {abs(train_loss - train_loss_eval):.6f}")
                    print(f"  –≠—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –≤–ª–∏—è–Ω–∏–µ dropout –∏–ª–∏ batch normalization!")
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º train –∏ val loss
            if epoch == 0 or (epoch + 1) % 10 == 0:
                if train_loss > val_loss * 2:
                    print(f"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}: Train Loss ({train_loss:.6f}) >> Val Loss ({val_loss:.6f})")
                    print(f"  –≠—Ç–æ –º–æ–∂–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ –ø—Ä–æ–±–ª–µ–º—É —Å –¥–∞–Ω–Ω—ã–º–∏ –∏–ª–∏ –º–æ–¥–µ–ª—å—é!")
                    print(f"  –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
                    print(f"    1. Train –∏ val –¥–∞–Ω–Ω—ã–µ –∏–º–µ—é—Ç —Ä–∞–∑–Ω—ã–µ –º–∞—Å—à—Ç–∞–±—ã (—Ä–∞–∑–Ω—ã–µ std)")
                    print(f"       ‚Üí –≠—Ç–æ –ù–û–†–ú–ê–õ–¨–ù–û –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤, –Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ loss")
                    print(f"       ‚Üí Train –¥–∞–Ω–Ω—ã–µ –º–æ–≥—É—Ç –∏–º–µ—Ç—å –±–æ–ª—å—à—É—é –≤–∞—Ä–∏–∞—Ü–∏—é, —á–µ–º val")
                    print(f"    2. –ú–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º–µ train() –≤–µ–¥–µ—Ç —Å–µ–±—è –ø–æ-–¥—Ä—É–≥–æ–º—É (dropout, batch norm)")
                    print(f"    3. Train –¥–∞–Ω–Ω—ã—Ö —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                    print(f"  –†–µ—à–µ–Ω–∏–µ:")
                    print(f"    - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É train –∏ val –¥–∞–Ω–Ω—ã—Ö (mean, std)")
                    print(f"    - –ï—Å–ª–∏ std —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è, —ç—Ç–æ –æ–±—ä—è—Å–Ω—è–µ—Ç —Ä–∞–∑–Ω–∏—Ü—É –≤ loss")
                    print(f"    - –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤, –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ–±–ª–µ–º–æ–π")
            
            # Scheduler step (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä)
            if self.scheduler is not None:
                old_lr = self.optimizer.param_groups[0]['lr']
                self.scheduler.step(val_loss)
                new_lr = self.optimizer.param_groups[0]['lr']
                if old_lr != new_lr and verbose:
                    print(f"Learning rate –∏–∑–º–µ–Ω–µ–Ω: {old_lr:.6f} -> {new_lr:.6f}")
            
            # Early stopping - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.patience_counter = 0
                self.best_model_state = self.model.state_dict().copy()
                self.best_epoch = epoch + 1
            else:
                self.patience_counter += 1
            
            if verbose and (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
            
            # Early stopping - –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è, –µ—Å–ª–∏ –Ω–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è
            if self.patience_counter >= self.early_stopping_patience:
                if verbose:
                    print(f"Early stopping at epoch {epoch+1} (–ª—É—á—à–∞—è –º–æ–¥–µ–ª—å –±—ã–ª–∞ –Ω–∞ —ç–ø–æ—Ö–µ {self.best_epoch})")
                break
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å (—Å –ª—É—á—à–∏–º validation loss)
        if self.best_model_state is not None:
            self.model.load_state_dict(self.best_model_state)
            if verbose:
                print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å —ç–ø–æ—Ö–∏ {self.best_epoch} (val_loss={self.best_val_loss:.4f})")
        else:
            # –ï—Å–ª–∏ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å –Ω–µ –±—ã–ª–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (–Ω–µ –¥–æ–ª–∂–Ω–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç—å), —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â—É—é
            if verbose:
                print("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å –Ω–µ –±—ã–ª–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω—è—è —ç–ø–æ—Ö–∞")
        
        return self.train_losses, self.val_losses
    
    def predict(self, test_loader):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
        self.model.eval()
        predictions = []
        targets = []
        
        with torch.no_grad():
            for X_batch, y_batch in test_loader:
                X_batch = X_batch.to(self.device)
                y_batch = y_batch.to(self.device)
                
                pred = self.model(X_batch)
                predictions.append(pred.cpu().numpy())
                targets.append(y_batch.cpu().numpy())
        
        return np.concatenate(predictions, axis=0), np.concatenate(targets, axis=0)


def train_model(model, X_train, y_train, X_val, y_val, 
                batch_size=32, epochs=100, device='cpu', verbose=True, **trainer_kwargs):
    """–£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏."""
    # –°–æ–∑–¥–∞–µ–º datasets
    train_dataset = TimeSeriesDataset(X_train, y_train)
    val_dataset = TimeSeriesDataset(X_val, y_val)
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è DataLoader –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, 
                             num_workers=0, pin_memory=False)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,
                           num_workers=0, pin_memory=False)
    
    # –°–æ–∑–¥–∞–µ–º trainer
    trainer = ModelTrainer(model, device=device, **trainer_kwargs)
    
    # –û–±—É—á–∞–µ–º
    train_losses, val_losses = trainer.train(train_loader, val_loader, epochs=epochs, verbose=verbose)
    
    return trainer, train_losses, val_losses

